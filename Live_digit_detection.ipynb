{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        channels_conv1 = 1\n",
    "        self.conv1 = nn.Conv2d(channels_conv1,10,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320,64)\n",
    "        self.fc2 = nn.Linear(64,10)\n",
    "        #self.CNN = nn.Sequential(\n",
    "        #    nn.Conv2d(channels_conv1,10,kernel_size=5),\n",
    "        #    nn.MaxPool2d(kernel_size=2),\n",
    "        #    nn.ReLU(),\n",
    "        #    #nn.Linear(64,10),\n",
    "        #    nn.Conv2d(10, 1, kernel_size=5),\n",
    "        #    nn.MaxPool2d(kernel_size=2),#F.max_pool2d(,5),\n",
    "        #    nn.ReLU(),#F.relu(),\n",
    "        #    #nn.Linear(10,64),\n",
    "        #    nn.Linear(4,10),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Softmax(),#F.log_softmax(),\n",
    "        #    \n",
    "        #)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x)\n",
    "        #x = self.CNN(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_config():\n",
    "\n",
    "    config = {\n",
    "          'batch_size_train':64,\n",
    "          'batch_size_test':1000,\n",
    "          'log_interval':40,      #How often to dislay (batch) loss during training\n",
    "          'epochs': 10,           #Number of epochs\n",
    "          'learning_rate': 0.0001,\n",
    "         }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, epoch, data_loader, optimizer, is_training, config):\n",
    "    if is_training==True: \n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0 \n",
    "    correct = 0 \n",
    "    labels_list = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "    for batch_idx, (features,target) in enumerate(data_loader):\n",
    "        \n",
    "        if not is_training:\n",
    "            with torch.no_grad():\n",
    "                prediction = model(features)\n",
    "                loss = F.nll_loss(prediction,target)\n",
    "                total_loss = total_loss + loss.detach().cpu().numpy()  \n",
    "            \n",
    "        elif is_training:\n",
    "            prediction = model(features)\n",
    "            loss = F.nll_loss(prediction,target)\n",
    "            total_loss = total_loss + loss.detach().cpu().numpy() \n",
    "\n",
    "            #Update gradients based on loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        # Compute the correct classification\n",
    "        predicted_label = np.zeros(prediction.shape)\n",
    "        predicted_label = np.argmax(prediction.detach().numpy(), axis=-1)\n",
    "    loss_avg = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(predicted_label, target)\n",
    "\n",
    "    print(f'Epoch={epoch} | loss = {total_loss/(batch_idx+1)} | accuracy = {accuracy}')\n",
    "    \n",
    "    return loss_avg, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(model,features):\n",
    "    with torch.no_grad():\n",
    "        prediction = model(features)\n",
    "        predicted_label = np.zeros(prediction.shape)\n",
    "        predicted_label = np.argmax(prediction.detach().numpy(), axis=-1)\n",
    "    pred = F.softmax(prediction)\n",
    "    pred = pred.detach().numpy()\n",
    "    return predicted_label, np.max(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_image(image, size):\n",
    "    U, s, V = np.linalg.svd(image,full_matrices=False)\n",
    "    op1 = np.dot(np.diag(s[:size]),V[:size,:])\n",
    "    compressed = np.dot(U[:,:size], op1)\n",
    "    return compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nothing(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " def draw_on_gui(event, x, y, flags, params): #params\n",
    "    global draw, window\n",
    "    \n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        draw=True\n",
    "        cv2.circle( window, (x,y), cv2.getTrackbarPos(\"Brush Size\", title), (255,255,255), -1 )\n",
    "        \n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if draw:\n",
    "            cv2.circle( window, (x,y), cv2.getTrackbarPos(\"Brush Size\", title), (255,255,255), -1 )\n",
    "    \n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        draw=False\n",
    "        cv2.circle( window, (x,y), cv2.getTrackbarPos(\"Brush Size\", title), (255,255,255), -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_window(*args):\n",
    "    cv2.rectangle(window, (0,0), (839,839), (0,0,0),-1)\n",
    "    return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_loop(title, window,model):\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        cv2.imshow(title, window)\n",
    "        key = cv2.waitKey(1)\n",
    "        #print(draw)\n",
    "        if key==ord(\"q\"):\n",
    "            break\n",
    "        \n",
    "        cur = cv2.resize(window, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "        cur = cur/255\n",
    "        \n",
    "        data = torch.tensor(cur)\n",
    "        data = data.double()\n",
    "        data.unsqueeze_(0).unsqueeze_(0)\n",
    "        pred, prob = run_prediction(model.double(),data)\n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print(pred, prob)\n",
    "        \n",
    "        if count == 1000:\n",
    "            count = 0\n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_from_batches(dataloader,k):\n",
    "    c = 1\n",
    "    for i, (image, target) in enumerate(dataloader):\n",
    "        plt.subplot(1, k, c)\n",
    "        plt.axis('off')\n",
    "        img = image[0,0,:,:]\n",
    "        plt.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        if c==k:\n",
    "            break\n",
    "        c+=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0 | loss = 0.8491878516352507 | accuracy = 0.84375\n",
      "Epoch=0 | loss = 0.29196321964263916 | accuracy = 0.926\n",
      "Epoch=1 | loss = 0.32674883481567857 | accuracy = 0.9375\n",
      "Epoch=1 | loss = 0.19671013206243515 | accuracy = 0.955\n",
      "Epoch=2 | loss = 0.2497076038628626 | accuracy = 0.96875\n",
      "Epoch=2 | loss = 0.15029880106449128 | accuracy = 0.956\n",
      "Epoch=3 | loss = 0.20995082911143678 | accuracy = 0.9375\n",
      "Epoch=3 | loss = 0.12362290546298027 | accuracy = 0.966\n",
      "Epoch=4 | loss = 0.18534400435621295 | accuracy = 0.9375\n",
      "Epoch=4 | loss = 0.11050927862524987 | accuracy = 0.968\n",
      "Epoch=5 | loss = 0.16893185483518122 | accuracy = 1.0\n",
      "Epoch=5 | loss = 0.10128072202205658 | accuracy = 0.968\n",
      "Epoch=6 | loss = 0.15726666529374972 | accuracy = 0.875\n",
      "Epoch=6 | loss = 0.09434628784656525 | accuracy = 0.964\n",
      "Epoch=7 | loss = 0.1458711491516436 | accuracy = 1.0\n",
      "Epoch=7 | loss = 0.08740199655294419 | accuracy = 0.977\n",
      "Epoch=8 | loss = 0.14137606458988652 | accuracy = 1.0\n",
      "Epoch=8 | loss = 0.0868711568415165 | accuracy = 0.962\n",
      "Epoch=9 | loss = 0.13603673338580297 | accuracy = 0.9375\n",
      "Epoch=9 | loss = 0.08414038866758347 | accuracy = 0.982\n"
     ]
    }
   ],
   "source": [
    "cconf = cnn_config()\n",
    "\n",
    "DL_train = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])), batch_size=cconf['batch_size_train'], shuffle=True)\n",
    "\n",
    "DL_test = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=cconf['batch_size_test'], shuffle=True)\n",
    "\n",
    "CNNmodel = CNNClassifier()\n",
    "optimizer = optim.Adam(CNNmodel.parameters(), lr=0.0001, weight_decay=0.01)#001)\n",
    "\n",
    "loss_train = np.zeros(shape=cconf['epochs'])\n",
    "acc_train = np.zeros(shape=cconf['epochs'])\n",
    "loss_test = np.zeros(shape=cconf['epochs'])\n",
    "acc_test = np.zeros(shape=cconf['epochs'])\n",
    "\n",
    "for epoch in range(cconf['epochs']):\n",
    "    loss_train[epoch],acc_train[epoch] = run_epoch(CNNmodel,epoch,DL_train,optimizer,True,cconf)\n",
    "    loss_test[epoch],acc_test[epoch] = run_epoch(CNNmodel,epoch,DL_test,optimizer,False,cconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images_from_batches(DL_train,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7] 0.12874086884015995\n",
      "[3] 0.1609397961296864\n",
      "[3] 0.304598561454685\n",
      "[7] 0.32608687808480363\n",
      "[7] 0.3454037546977105\n",
      "[7] 0.3454037546977105\n",
      "[7] 0.3630105529000046\n",
      "[7] 0.38825113370842906\n",
      "[7] 0.405315148595426\n",
      "[7] 0.405315148595426\n",
      "[7] 0.405315148595426\n",
      "[7] 0.405315148595426\n"
     ]
    }
   ],
   "source": [
    "title = \"Neural Network Guesser\"\n",
    "draw = False\n",
    "mat = np.zeros((840,840),np.uint8)#np.ones((840,840),np.uint8)*255\n",
    "\n",
    "cv2.namedWindow(title)\n",
    "\n",
    "window = cv2.rectangle(mat, (0,0), (839,839), (0,0,0),-1)\n",
    "cv2.createTrackbar(\"Brush Size\", title, 33, 50, nothing)\n",
    "cv2.createButton(\"Clear\", clear_window)\n",
    "cv2.setMouseCallback(title, draw_on_gui)\n",
    "update_loop(title, window, CNNmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d061059c50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(window, cmap=plt.cm.gray_r, interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: Make True GUI buttons, make a numberdisplay that updates instead of a print, \n",
    "#make a function to save image with corrsponding targetlabel, make it web\n",
    "#clean all code, make orderly filesystem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
